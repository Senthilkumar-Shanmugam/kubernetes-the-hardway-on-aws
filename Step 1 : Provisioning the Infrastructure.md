# kubernetes-the-hardway-on-aws
my attempt in doing it for CKA prep

Prerequsites:

1.AWS free tier account
2.AWS CLI/AWS Shell
3.Open SSL/CFFSSL


1. Creating Compute Instances
==============================

SSH Key Pair
============

mkdir -p ssh

aws ec2 create-key-pair \
  --key-name kubernetes \
  --output text --query 'KeyMaterial' \
  > ssh/kubernetes.id_rsa
  
  Or
  
  aws ec2 create-key-pair \
  --key-name kubernetes-the-hard-way \
  --query 'KeyMaterial' \
  --output text > ~/.ssh/kubernetes-the-hard-way.pem
  
chmod 600 ssh/kubernetes.id_rsa  

Networking
===========
1. Create VPC
==============
   VPC_ID=$(aws ec2 create-vpc --cidr-block 10.240.0.0/24 --output text --query 'Vpc.VpcId')
   
   aws ec2 create-tags --resources ${VPC_ID} --tags Key=Name,Value=kubernetes-the-hard-way
   
   aws ec2 modify-vpc-attribute --vpc-id ${VPC_ID} --enable-dns-support '{"Value": true}'
   
   aws ec2 modify-vpc-attribute --vpc-id ${VPC_ID} --enable-dns-hostnames '{"Value": true}'
   
 2.Subnet
 ==========
 A subnet must be provisioned with an IP address range large enough to assign a private IP address to each node in the Kubernetes cluster.
 
  SUBNET_ID=$(aws ec2 create-subnet \
    --vpc-id ${VPC_ID} \
    --cidr-block 10.240.0.0/24 \
    --output text --query 'Subnet.SubnetId')
  
  aws ec2 create-tags --resources ${SUBNET_ID} --tags Key=Name,Value=kubernetes
  
  The 10.240.0.0/24 IP address range can host up to 254 compute instances.
  
3. Internet Gateway
==================
Create an internet gateway and attach it to the kubernetes-the-hard-way VPC:

  INTERNET_GATEWAY_ID=$(aws ec2 create-internet-gateway --output text --query 'InternetGateway.InternetGatewayId')
  
  aws ec2 create-tags --resources ${INTERNET_GATEWAY_ID} --tags Key=Name,Value=kubernetes
  
  aws ec2 attach-internet-gateway --internet-gateway-id ${INTERNET_GATEWAY_ID} --vpc-id ${VPC_ID}

4.Route Tables
==============

Create a route in the route table that points all traffic (0.0.0.0/0) to the Internet gateway:


ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id ${VPC_ID} --output text --query 'RouteTable.RouteTableId')

aws ec2 create-tags --resources ${ROUTE_TABLE_ID} --tags Key=Name,Value=kubernetes

aws ec2 create-route --route-table-id ${ROUTE_TABLE_ID} --destination-cidr-block 0.0.0.0/0 --gateway-id ${INTERNET_GATEWAY_ID}

Associate route table with the kubernetes-the-hard-way subnet:

aws ec2 associate-route-table --route-table-id ${ROUTE_TABLE_ID} --subnet-id ${SUBNET_ID}

Modify the public IP addressing behavior of your subnet so that an instance launched into the subnet automatically receives a public IP address:

aws ec2 modify-subnet-attribute \
  --subnet-id ${SUBNET_ID} \
  --map-public-ip-on-launch


5. Security Groups (aka Firewall Rules)
==========================================
A security group was generated by default during the creation of the kubernetes-the-hard-way vpc, which allows internal communication across all protocols. We will leave that security group and create a new one for the VPC.


SECURITY_GROUP_ID=$(aws ec2 create-security-group \
  --group-name kubernetes \
  --description "Kubernetes security group" \
  --vpc-id ${VPC_ID} \
  --output text --query 'GroupId')
  
aws ec2 create-tags --resources ${SECURITY_GROUP_ID} --tags Key=Name,Value=kubernetes
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol all --cidr 10.240.0.0/24
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol all --cidr 10.200.0.0/16
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 22 --cidr 0.0.0.0/0
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 6443 --cidr 0.0.0.0/0
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol tcp --port 443 --cidr 0.0.0.0/0
aws ec2 authorize-security-group-ingress --group-id ${SECURITY_GROUP_ID} --protocol icmp --port -1 --cidr 0.0.0.0/0


Provisioning AWS Load Balancer to Front HA Master Node API server
===================================================================

Since ELB is not free, I am going to provision a separate EC2 instance and run HA proxy on it.For fisrt time, I will provsion a ELB , once I get it to working, I will get rid of that and settup a HA proxy in EC2.

Kubernetes Public IP Address
===============================
Allocate a static IP address that will be attached to the external load balancer fronting the Kubernetes API Servers:

ELASTIC_IP=$(aws ec2 allocate-address \
  --domain vpc | jq -r '.AllocationId')
  
aws ec2 create-tags \
  --resources ${ELASTIC_IP} \
  --tags 'Key=Name,Value=kubernetes-the-hard-way'
Verify the kubernetes-the-hard-way static IP address was created in your default compute region:

aws ec2 describe-addresses \
  --output table \
  --filters '{"Name":"tag:Name", "Values":["kubernetes-the-hard-way"]}'

Output

----------------------------------------------------
|                 DescribeAddresses                |
+--------------------------------------------------+
||                    Addresses                   ||
|+--------------------+---------+-----------------+|
||    AllocationId    | Domain  |    PublicIp     ||
|+--------------------+---------+-----------------+|
||  eipalloc-XXXXXXXX |  vpc    |  XX.XXX.XX.XXX  ||
|+--------------------+---------+-----------------+|
|||                     Tags                     |||
||+---------+------------------------------------+||
|||   Key   |               Value                |||
||+---------+------------------------------------+||
|||  Name   |  kubernetes-the-hard-way           |||
||+---------+------------------------------------+||

Create Frontend Load Balancer
=================================
Create an external load balancer fronting the Kubernetes API Servers:

LOAD_BALANCER_ARN=$(aws elbv2 create-load-balancer \
  --name kubernetes-the-hard-way \
  --type network \
  --subnet-mappings "SubnetId=${SUBNET_ID},AllocationId=${ELASTIC_IP}" | jq -r '.LoadBalancers[].LoadBalancerArn')
  
TARGET_GROUP_ARN=$(aws elbv2 create-target-group \
  --name kubernetes-targets \
  --protocol TCP \
  --port 6443 \
  --vpc-id ${VPC_ID} | jq -r '.TargetGroups[].TargetGroupArn')
  
aws elbv2 create-listener \
  --load-balancer-arn ${LOAD_BALANCER_ARN} \
  --protocol TCP --port 6443 \
  --default-actions Type=forward,TargetGroupArn=${TARGET_GROUP_ARN}
  
Verify the kubernetes-the-hard-way elastic load balancer was created in your default compute region:

aws elbv2 describe-load-balancers \
  --output table \
  --names kubernetes-the-hard-way \
  --query "LoadBalancers[*].{Name:LoadBalancerName,Scheme:Scheme,State:State.Code,Public_IP:AvailabilityZones[*].LoadBalancerAddresses[*].IpAddress,Zone:AvailabilityZones[*].ZoneName}"
  
Output

----------------------------------------------------------
|                  DescribeLoadBalancers                 |
+--------------------------+-------------------+---------+
|           Name           |      Scheme       |  State  |
+--------------------------+-------------------+---------+
|  kubernetes-the-hard-way |  internet-facing  |  active |
+--------------------------+-------------------+---------+
||                       Public_IP                      ||
|+------------------------------------------------------+|
||  XX.XXX.XX.XXX                                       ||
|+------------------------------------------------------+|
||                         Zone                         ||
|+------------------------------------------------------+|
||  us-east-1b                                          ||
|+------------------------------------------------------+|


  
Instance Image
==============

IMAGE_ID=$(aws ec2 describe-images --owners 099720109477 \
  --filters \
  'Name=root-device-type,Values=ebs' \
  'Name=architecture,Values=x86_64' \
  'Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-*' \
  | jq -r '.Images|sort_by(.Name)[-1]|.ImageId')   
   


Kubernetes Controllers
=======================

Using t2.micro instead of t2.small as t2.micro is covered by AWS free tier

for i in 0 1 2; do
  instance_id=$(aws ec2 run-instances \
    --associate-public-ip-address \
    --image-id ${IMAGE_ID} \
    --count 1 \
    --key-name kubernetes \
    --security-group-ids ${SECURITY_GROUP_ID} \
    --instance-type t2.micro \
    --private-ip-address 10.240.0.1${i} \
    --user-data "name=controller-${i}" \
    --subnet-id ${SUBNET_ID} \
    --output text --query 'Instances[].InstanceId')
  aws ec2 modify-instance-attribute \
    --instance-id ${instance_id} \
    --no-source-dest-check
  aws ec2 create-tags \
    --resources ${instance_id} \
    --tags "Key=Name,Value=controller-${i}"
done

Kubernetes Workers
===================
 
for i in 0 1 2; do
  instance_id=$(aws ec2 run-instances \
    --associate-public-ip-address \
    --image-id ${IMAGE_ID} \
    --count 1 \
    --key-name kubernetes \
    --security-group-ids ${SECURITY_GROUP_ID} \
    --instance-type t2.micro \
    --private-ip-address 10.240.0.2${i} \
    --user-data "name=worker-${i}|pod-cidr=10.200.${i}.0/24" \
    --subnet-id ${SUBNET_ID} \
    --output text --query 'Instances[].InstanceId')
  aws ec2 modify-instance-attribute \
    --instance-id ${instance_id} \
    --no-source-dest-check
  aws ec2 create-tags \
    --resources ${instance_id} \
    --tags "Key=Name,Value=worker-${i}"
done

Verification
===============

List the compute instances in your kubernetes-the-hard-way VPC:

aws ec2 describe-instances \
  --output table \
  --filters "Name=vpc-id,Values=${VPC_ID}" \
  --query "Reservations[*].Instances[*].{Zone:Placement.AvailabilityZone,Instance_Type:InstanceType,Private_IP:PrivateIpAddress,Public_IP:PublicIpAddress,Status:State.Name,Name:Tags[?Key=='Name']|[0].Value}"

---------------------------------------------------------------------------------------------
|                                     DescribeInstances                                     |
+---------------+---------------+--------------+-----------------+-----------+--------------+
| Instance_Type |     Name      | Private_IP   |    Public_IP    |  Status   |    Zone      |
+---------------+---------------+--------------+-----------------+-----------+--------------+
|  t2.micro     |  controller-1 |  10.240.0.11 |  3.87.103.54    |  running  |  us-east-1d  |
|  t2.micro     |  controller-0 |  10.240.0.10 |  52.207.140.83  |  running  |  us-east-1d  |
|  t2.micro     |  worker-0     |  10.240.0.20 |  3.90.174.174   |  running  |  us-east-1d  |
|  t2.micro     |  worker-1     |  10.240.0.21 |  18.232.122.188 |  running  |  us-east-1d  |
|  t2.micro     |  worker-2     |  10.240.0.22 |  54.165.120.42  |  running  |  us-east-1d  |
|  t2.micro     |  controller-2 |  10.240.0.12 |  54.165.186.224 |  running  |  us-east-1d  |
+---------------+---------------+--------------+-----------------+-----------+--------------+

Once instances are created verify that you are able to login into all 6 instances. It also has docker version 19 in all nodes.

ssh -i ssh/{public key} ubuntu@{PUBLIC_IP}
